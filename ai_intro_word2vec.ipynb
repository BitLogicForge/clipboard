{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings and Neural Networks Introduction\n",
    "\n",
    "This notebook demonstrates the fundamentals of natural language processing using Word2Vec and a simple neural network. We'll walk through:\n",
    "\n",
    "1. Basic text preprocessing\n",
    "2. Word embedding with Word2Vec\n",
    "3. Building a simple neural network with PyTorch\n",
    "4. Making predictions with the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\super\\miniconda3\\envs\\nlp_env\\lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting with a Simple Sentence\n",
    "\n",
    "We'll begin with a basic Polish sentence to demonstrate the NLP pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example sentence\n",
    "sentence = [\"kot goni psa\"]\n",
    "print(f\"Our sample sentence: '{sentence[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is the process of converting words into numerical tokens that computers can process. We create a simple vocabulary mapping each word to a unique integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary dictionary mapping words to unique integers\n",
    "vocab = {\"kot\": 0, \"goni\": 1, \"psa\": 2}\n",
    "\n",
    "# Convert each word in our sentence to its numerical token\n",
    "tokens = [vocab[word] for word in sentence[0].split()]\n",
    "print(f\"Words: {sentence[0].split()}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embeddings with Word2Vec\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous vector space where semantically similar words are mapped close to each other. Word2Vec is a popular method for generating these embeddings.\n",
    "\n",
    "Here we train a Word2Vec model on a small corpus of sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training corpus (collection of sentences)\n",
    "sentences = [[\"kot\", \"goni\", \"psa\"], [\"pies\", \"goni\", \"kota\"], [\"ryba\", \"p≈Çywa\", \"w\", \"wodzie\"]]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# Parameters:\n",
    "# - vector_size: dimension of the word vectors\n",
    "# - window: context window size (words before and after)\n",
    "# - min_count: ignore words with fewer occurrences\n",
    "# - sg=0: use CBOW architecture (sg=1 would use Skip-gram)\n",
    "word2vec_model = Word2Vec(sentences, vector_size=3, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Extract vectors for words in our vocabulary\n",
    "word_vectors = {word: word2vec_model.wv[word] for word in vocab.keys()}\n",
    "\n",
    "# Display the vector for each word\n",
    "for word, vector in word_vectors.items():\n",
    "    print(f\"{word}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Embeddings\n",
    "\n",
    "Let's visualize our word embeddings to better understand how words are positioned in the vector space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words from the trained model\n",
    "all_words = list(word2vec_model.wv.key_to_index.keys())\n",
    "all_vectors = [word2vec_model.wv[word] for word in all_words]\n",
    "\n",
    "# Since we used vector_size=3, we can use PCA to visualize in 2D\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(all_vectors)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(result[:, 0], result[:, 1], marker=\"o\")\n",
    "\n",
    "# Add labels for each word\n",
    "for i, word in enumerate(all_words):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(result[i, 0], result[i, 1]),\n",
    "        xytext=(5, 2),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Word Embeddings Visualization using PCA\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Tokens to Vectors\n",
    "\n",
    "Now we'll transform our tokenized sentence into a sequence of vectors that can be processed by a neural network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each word in our original sentence to its vector representation\n",
    "input_vectors = np.array([word_vectors[word] for word in sentence[0].split()])\n",
    "print(f\"Input vectors shape: {input_vectors.shape}\")\n",
    "print(\"Input vectors:\")\n",
    "for i, word in enumerate(sentence[0].split()):\n",
    "    print(f\"{word}: {input_vectors[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Simple Neural Network\n",
    "\n",
    "We'll create a simple neural network that takes word vectors as input and predicts the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # A single linear (fully connected) layer\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        # You could add more complexity with additional layers:\n",
    "        # self.hidden = nn.Linear(input_dim, 64)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.output = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simple forward pass with just one layer\n",
    "        return self.fc(x)\n",
    "\n",
    "        # With more layers, you'd do:\n",
    "        # x = self.hidden(x)\n",
    "        # x = self.relu(x)\n",
    "        # return self.output(x)\n",
    "\n",
    "\n",
    "# Initialize the network\n",
    "input_dim = 3  # Dimensionality of our word vectors\n",
    "output_dim = len(vocab)  # Number of possible output words\n",
    "model = SimpleNN(input_dim, output_dim)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Forward Pass and Prediction\n",
    "\n",
    "Now we'll run our word vectors through the neural network and generate predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "input_tensor = torch.tensor(input_vectors, dtype=torch.float32)\n",
    "\n",
    "# Run the input through the model\n",
    "output = model(input_tensor)\n",
    "print(\"Raw neural network output:\")\n",
    "print(output)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probabilities = softmax(output)\n",
    "print(\"\\nProbabilities for each word in vocabulary:\")\n",
    "print(probabilities)\n",
    "\n",
    "# Find the most likely next word for each input word\n",
    "predicted_tokens = torch.argmax(probabilities, dim=1)\n",
    "predicted_words = [list(vocab.keys())[token.item()] for token in predicted_tokens]\n",
    "\n",
    "print(\"\\nPredicted next word for each input word:\")\n",
    "for i, word in enumerate(sentence[0].split()):\n",
    "    print(f\"After '{word}': '{predicted_words[i]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Model's Decision Process\n",
    "\n",
    "Let's create a bar chart to better visualize the prediction probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last word's prediction probabilities\n",
    "last_word_probs = probabilities[-1].detach().numpy()\n",
    "\n",
    "# Plot as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(list(vocab.keys()), last_word_probs, color=\"skyblue\")\n",
    "plt.title(f\"Prediction Probabilities After the Word '{sentence[0].split()[-1]}'\")\n",
    "plt.xlabel(\"Possible Next Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered the fundamental steps of natural language processing:\n",
    "\n",
    "1. **Tokenization**: Converting words to numerical tokens\n",
    "2. **Word Embeddings**: Representing words as vectors using Word2Vec\n",
    "3. **Neural Network**: Building a simple predictive model with PyTorch\n",
    "4. **Prediction**: Generating probabilities for the next word\n",
    "\n",
    "Note that in a real-world scenario:\n",
    "\n",
    "- We would use much larger training datasets\n",
    "- Our vocabulary would be much more extensive\n",
    "- Word vectors would have higher dimensions (typically 100-300)\n",
    "- The neural network would be more complex (e.g., an LSTM or Transformer)\n",
    "- We would properly train the model with a loss function and optimizer\n",
    "\n",
    "This notebook serves as a simplified introduction to the concepts of word embeddings and neural networks for NLP.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
