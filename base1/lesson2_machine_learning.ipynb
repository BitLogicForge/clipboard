{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Machine Learning (ML): The Engine of AI\n",
    "\n",
    "Welcome to the second lesson in our AI course! Now that we understand what AI is, let's explore Machine Learning - the technology that powers most modern AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "**Machine Learning (ML)** is a subset of artificial intelligence that focuses on developing systems that learn from data, identify patterns, and make decisions with minimal human intervention.\n",
    "\n",
    "In traditional programming, we provide both the data and the rules to get answers:\n",
    "\n",
    "```\n",
    "Data + Rules → Answers\n",
    "```\n",
    "\n",
    "In machine learning, we provide data and answers to get rules:\n",
    "\n",
    "```\n",
    "Data + Answers → Rules\n",
    "```\n",
    "\n",
    "Then we can use these learned rules on new data:\n",
    "\n",
    "```\n",
    "New Data + Rules → New Answers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison: Traditional Programming vs. Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle, Arrow\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Traditional Programming\n",
    "ax1.set_title(\"Traditional Programming\", fontsize=16)\n",
    "ax1.add_patch(Rectangle((0.1, 0.7), 0.3, 0.2, facecolor=\"lightblue\", edgecolor=\"black\"))\n",
    "ax1.text(0.25, 0.8, \"Data\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax1.add_patch(Rectangle((0.1, 0.4), 0.3, 0.2, facecolor=\"lightgreen\", edgecolor=\"black\"))\n",
    "ax1.text(0.25, 0.5, \"Rules\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax1.add_patch(Rectangle((0.6, 0.55), 0.3, 0.2, facecolor=\"lightsalmon\", edgecolor=\"black\"))\n",
    "ax1.text(0.75, 0.65, \"Answers\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax1.add_patch(Arrow(0.45, 0.65, 0.1, 0, width=0.05, facecolor=\"black\"))\n",
    "ax1.text(0.5, 0.7, \"Program\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Machine Learning\n",
    "ax2.set_title(\"Machine Learning\", fontsize=16)\n",
    "ax2.add_patch(Rectangle((0.1, 0.7), 0.3, 0.2, facecolor=\"lightblue\", edgecolor=\"black\"))\n",
    "ax2.text(0.25, 0.8, \"Data\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax2.add_patch(Rectangle((0.1, 0.4), 0.3, 0.2, facecolor=\"lightsalmon\", edgecolor=\"black\"))\n",
    "ax2.text(0.25, 0.5, \"Answers\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax2.add_patch(Rectangle((0.6, 0.55), 0.3, 0.2, facecolor=\"lightgreen\", edgecolor=\"black\"))\n",
    "ax2.text(0.75, 0.65, \"Rules\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "ax2.add_patch(Arrow(0.45, 0.65, 0.1, 0, width=0.05, facecolor=\"black\"))\n",
    "ax2.text(0.5, 0.7, \"Learning\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Hide axes\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Machine Learning Concepts\n",
    "\n",
    "### Data\n",
    "\n",
    "The information used to train the model, typically organized in datasets.\n",
    "\n",
    "### Model\n",
    "\n",
    "A mathematical representation of a real-world process that can make predictions or decisions.\n",
    "\n",
    "### Training\n",
    "\n",
    "The process of teaching a model by showing it examples and adjusting it to improve performance.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "Using a trained model to generate outputs for new, unseen inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "There are three main categories of machine learning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Supervised Learning\n",
    "\n",
    "In supervised learning, the algorithm learns from labeled data. Like a student learning with an answer key.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- The training data includes both inputs and correct outputs (labels)\n",
    "- The model learns to map inputs to outputs\n",
    "- Once trained, it can predict outputs for new inputs\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Classification: Spam detection, image recognition\n",
    "- Regression: House price prediction, weather forecasting\n",
    "\n",
    "**Keywords:** Labels, ground truth, target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Supervised Learning Example: Linear Regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data: house size vs. price\n",
    "np.random.seed(42)\n",
    "house_size = np.random.randint(1000, 5000, 100).reshape(-1, 1)  # Square feet (feature)\n",
    "house_price = (\n",
    "    100000 + 200 * house_size + np.random.normal(0, 25000, 100).reshape(-1, 1)\n",
    ")  # Price in $ (target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = 70\n",
    "X_train, X_test = house_size[:train_size], house_size[train_size:]\n",
    "y_train, y_test = house_price[:train_size], house_price[train_size:]\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate error\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "# Print model parameters and performance\n",
    "print(f\"Model Slope (Price per sq ft): ${model.coef_[0][0]:.2f}\")\n",
    "print(f\"Model Intercept (Base price): ${model.intercept_[0]:.2f}\")\n",
    "print(f\"Training Mean Squared Error: ${train_mse:.2f}\")\n",
    "print(f\"Testing Mean Squared Error: ${test_mse:.2f}\")\n",
    "\n",
    "# Visualize the data and model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color=\"blue\", alpha=0.6, label=\"Training Data\")\n",
    "plt.scatter(X_test, y_test, color=\"green\", alpha=0.6, label=\"Testing Data\")\n",
    "\n",
    "# Plot the prediction line\n",
    "plt.plot(\n",
    "    [min(house_size), max(house_size)],\n",
    "    [\n",
    "        model.predict(min(house_size).reshape(1, -1))[0],\n",
    "        model.predict(max(house_size).reshape(1, -1))[0],\n",
    "    ],\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    label=\"Model Prediction\",\n",
    ")\n",
    "\n",
    "plt.title(\"House Price Prediction: Supervised Learning Example\", fontsize=16)\n",
    "plt.xlabel(\"House Size (square feet)\", fontsize=14)\n",
    "plt.ylabel(\"Price ($)\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, the algorithm learns from unlabeled data, finding hidden patterns or structures.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- The training data includes only inputs (no labels)\n",
    "- The model discovers patterns, groupings, or structures in the data\n",
    "- Useful for exploration and finding hidden insights\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Clustering: Customer segmentation, anomaly detection\n",
    "- Dimensionality reduction: Feature extraction, visualization\n",
    "- Association: Market basket analysis\n",
    "\n",
    "**Keywords:** Clustering, patterns, similarity, dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Unsupervised Learning Example: K-Means Clustering\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data for clustering\n",
    "np.random.seed(42)\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Create a K-means clustering model and fit it to the data\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster centers and labels\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot original data points colored by cluster\n",
    "for i in range(4):\n",
    "    plt.scatter(X[labels == i, 0], X[labels == i, 1], s=50, alpha=0.7, label=f\"Cluster {i+1}\")\n",
    "\n",
    "# Plot cluster centers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=200, c=\"black\", marker=\"X\", label=\"Centroids\")\n",
    "\n",
    "plt.title(\"K-Means Clustering: Unsupervised Learning Example\", fontsize=16)\n",
    "plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "plt.ylabel(\"Feature 2\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The K-means algorithm identified 4 clusters without any labels.\")\n",
    "print(\"This demonstrates how unsupervised learning can find patterns in unlabeled data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, an agent learns to make decisions by taking actions and receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- An agent interacts with an environment\n",
    "- The agent takes actions and receives rewards (or penalties)\n",
    "- The agent learns which actions maximize cumulative rewards over time\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Game playing: AlphaGo, Chess AI\n",
    "- Robotics: Robot navigation, manipulation\n",
    "- Resource management: Traffic light control, data center optimization\n",
    "\n",
    "**Keywords:** Agent, environment, reward, policy, state, action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Reinforcement Learning Example: Multi-armed Bandit Problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, n_arms=4):\n",
    "        self.n_arms = n_arms\n",
    "        # True reward probabilities for each arm (unknown to the agent)\n",
    "        self.true_rewards = np.random.beta(2, 2, n_arms)\n",
    "\n",
    "    def pull(self, arm):\n",
    "        # Return 1 (reward) or 0 (no reward) based on the arm's probability\n",
    "        return 1 if np.random.random() < self.true_rewards[arm] else 0\n",
    "\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "    def __init__(self, n_arms=4, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        # Initialize estimates of reward for each arm\n",
    "        self.reward_estimates = np.zeros(n_arms)\n",
    "        # Count of pulls for each arm\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        # History for visualization\n",
    "        self.history = {\"rewards\": [], \"arm_choices\": [], \"estimates\": []}\n",
    "\n",
    "    def choose_arm(self):\n",
    "        # Exploration: random arm with probability epsilon\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        # Exploitation: arm with highest estimated reward\n",
    "        else:\n",
    "            return np.argmax(self.reward_estimates)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # Update the count for the selected arm\n",
    "        self.arm_counts[arm] += 1\n",
    "        # Update the reward estimate using running average\n",
    "        n = self.arm_counts[arm]\n",
    "        old_estimate = self.reward_estimates[arm]\n",
    "        self.reward_estimates[arm] = old_estimate + (1 / n) * (reward - old_estimate)\n",
    "\n",
    "        # Record history for visualization\n",
    "        self.history[\"rewards\"].append(reward)\n",
    "        self.history[\"arm_choices\"].append(arm)\n",
    "        self.history[\"estimates\"].append(self.reward_estimates.copy())\n",
    "\n",
    "\n",
    "# Set up the environment and agent\n",
    "np.random.seed(42)\n",
    "env = BanditEnvironment(n_arms=4)\n",
    "agent = EpsilonGreedyAgent(n_arms=4, epsilon=0.1)\n",
    "\n",
    "# Print the true reward probabilities (normally unknown to the agent)\n",
    "print(\"True reward probabilities for each arm:\")\n",
    "for i, prob in enumerate(env.true_rewards):\n",
    "    print(f\"Arm {i+1}: {prob:.3f}\")\n",
    "\n",
    "# Run the agent for 500 steps\n",
    "n_steps = 500\n",
    "for step in range(n_steps):\n",
    "    # Agent chooses an arm\n",
    "    chosen_arm = agent.choose_arm()\n",
    "    # Agent pulls the arm and receives reward\n",
    "    reward = env.pull(chosen_arm)\n",
    "    # Agent updates its knowledge based on the reward\n",
    "    agent.update(chosen_arm, reward)\n",
    "\n",
    "# Calculate cumulative rewards and running average reward\n",
    "cumulative_rewards = np.cumsum(agent.history[\"rewards\"])\n",
    "avg_rewards = [\n",
    "    sum(agent.history[\"rewards\"][: i + 1]) / (i + 1) if i > 0 else 0\n",
    "    for i in range(len(agent.history[\"rewards\"]))\n",
    "]\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Arm selection over time\n",
    "plt.subplot(2, 2, 1)\n",
    "arm_choices = np.array(agent.history[\"arm_choices\"])\n",
    "for arm in range(env.n_arms):\n",
    "    plt.plot(\n",
    "        np.where(arm_choices == arm)[0],\n",
    "        np.ones(np.sum(arm_choices == arm)) * arm,\n",
    "        \"|\",\n",
    "        markersize=10,\n",
    "        label=f\"Arm {arm+1}\",\n",
    "    )\n",
    "plt.yticks(range(env.n_arms), [f\"Arm {i+1}\" for i in range(env.n_arms)])\n",
    "plt.title(\"Arm Selection Over Time\", fontsize=14)\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Selected Arm\", fontsize=12)\n",
    "\n",
    "# Plot 2: Reward estimates over time\n",
    "plt.subplot(2, 2, 2)\n",
    "estimates = np.array(agent.history[\"estimates\"])\n",
    "for arm in range(env.n_arms):\n",
    "    plt.plot(estimates[:, arm], label=f\"Arm {arm+1} Est.\")\n",
    "plt.axhline(y=np.max(env.true_rewards), color=\"r\", linestyle=\"--\", label=\"Best Arm Prob.\")\n",
    "plt.title(\"Reward Estimates vs. Time\", fontsize=14)\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Estimated Reward Probability\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Cumulative reward\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.title(\"Cumulative Reward\", fontsize=14)\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Total Reward\", fontsize=12)\n",
    "\n",
    "# Plot 4: Average reward over time\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(avg_rewards)\n",
    "plt.axhline(y=np.max(env.true_rewards), color=\"r\", linestyle=\"--\", label=\"Best Arm Prob.\")\n",
    "plt.title(\"Average Reward vs. Time\", fontsize=14)\n",
    "plt.xlabel(\"Step\", fontsize=12)\n",
    "plt.ylabel(\"Average Reward\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal reward estimates:\")\n",
    "for i, est in enumerate(agent.reward_estimates):\n",
    "    print(f\"Arm {i+1}: {est:.3f} (true: {env.true_rewards[i]:.3f})\")\n",
    "\n",
    "print(f\"\\nBest arm based on agent's estimates: Arm {np.argmax(agent.reward_estimates)+1}\")\n",
    "print(f\"True best arm: Arm {np.argmax(env.true_rewards)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Machine Learning Algorithms\n",
    "\n",
    "Let's explore some of the most commonly used machine learning algorithms:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\n",
    "\n",
    "**Type**: Supervised Learning (Regression)\n",
    "\n",
    "**How it works**: Models the relationship between inputs and a continuous output variable by fitting a linear equation.\n",
    "\n",
    "**Use cases**:\n",
    "\n",
    "- Price prediction\n",
    "- Financial forecasting\n",
    "- Resource demand prediction\n",
    "\n",
    "**Keywords**: Coefficient, slope, intercept, least squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Trees\n",
    "\n",
    "**Type**: Supervised Learning (Classification or Regression)\n",
    "\n",
    "**How it works**: Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "**Use cases**:\n",
    "\n",
    "- Customer segmentation\n",
    "- Medical diagnosis\n",
    "- Credit risk assessment\n",
    "\n",
    "**Keywords**: Root, nodes, leaves, branches, pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train a decision tree classifier\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_tree(\n",
    "    dt_clf,\n",
    "    filled=True,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    ")\n",
    "plt.title(\"Decision Tree for Iris Classification\", fontsize=14)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=iris.target_names,\n",
    "    yticklabels=iris.target_names,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display feature importance\n",
    "feature_importance = dt_clf.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [iris.feature_names[i] for i in sorted_idx])\n",
    "plt.title(\"Feature Importance in Decision Tree\", fontsize=14)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Networks\n",
    "\n",
    "**Type**: Supervised, Unsupervised, or Reinforcement Learning\n",
    "\n",
    "**How it works**: Inspired by the human brain, uses interconnected nodes (neurons) in multiple layers to learn complex patterns.\n",
    "\n",
    "**Use cases**:\n",
    "\n",
    "- Image and speech recognition\n",
    "- Natural language processing\n",
    "- Game playing\n",
    "\n",
    "**Keywords**: Neurons, layers, activation function, backpropagation\n",
    "\n",
    "_Note: We'll dive deeper into neural networks in our next lesson on Deep Learning._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Challenges in Machine Learning\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "**What is it?** When a model learns the training data too well, including its noise and outliers, causing poor performance on new data.\n",
    "\n",
    "**Signs of overfitting:**\n",
    "\n",
    "- High accuracy on training data but low accuracy on test data\n",
    "- Model is too complex for the problem\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "- More training data\n",
    "- Regularization techniques\n",
    "- Simpler models\n",
    "- Cross-validation\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "**What is it?** When a model is too simple to capture the underlying pattern in the data.\n",
    "\n",
    "**Signs of underfitting:**\n",
    "\n",
    "- Low accuracy on both training and test data\n",
    "- Model makes oversimplified assumptions\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "- More complex models\n",
    "- Additional features\n",
    "- Reducing regularization\n",
    "\n",
    "### Accuracy vs. Complexity Trade-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing overfitting and underfitting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.uniform(0, 1, 30))\n",
    "y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, 30)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = X[:20], X[20:]\n",
    "y_train, y_test = y[:20], y[20:]\n",
    "\n",
    "# Create true function for comparison\n",
    "X_plot = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "y_true = np.sin(2 * np.pi * X_plot)\n",
    "\n",
    "# Create models with different complexity levels\n",
    "degrees = [1, 3, 15]  # Underfitting, Good fit, Overfitting\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "\n",
    "    # Create polynomial features and fit model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate training and testing scores\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "\n",
    "    # Predict on the plotting range\n",
    "    y_plot = model.predict(X_plot)\n",
    "\n",
    "    # Plot the results\n",
    "    ax.plot(X_plot, y_true, \"k--\", label=\"True function\")\n",
    "    ax.plot(X_plot, y_plot, \"r-\", label=f\"Degree {degree} polynomial\")\n",
    "    ax.scatter(X_train, y_train, c=\"b\", s=50, alpha=0.7, label=\"Training data\")\n",
    "    ax.scatter(X_test, y_test, c=\"g\", s=50, alpha=0.7, label=\"Testing data\")\n",
    "\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "\n",
    "    if degree == 1:\n",
    "        title = \"Underfitting: Degree 1 Polynomial\"\n",
    "    elif degree == 15:\n",
    "        title = \"Overfitting: Degree 15 Polynomial\"\n",
    "    else:\n",
    "        title = \"Good Fit: Degree 3 Polynomial\"\n",
    "\n",
    "    ax.set_title(f\"{title}\\nTrain score: {train_score:.2f}, Test score: {test_score:.2f}\")\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Exercise: Identify Types of ML\n",
    "\n",
    "For each of the following scenarios, identify which type of machine learning would be most appropriate:\n",
    "\n",
    "1. Predicting stock prices based on historical data\n",
    "2. Grouping customers based on purchasing behavior\n",
    "3. Teaching a robot to play chess\n",
    "4. Identifying spam emails based on labeled examples\n",
    "5. Discovering patterns in genetic data without prior knowledge\n",
    "6. Building a recommendation system for movies\n",
    "\n",
    "Think about your answers before checking the solutions below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    \"Predicting stock prices based on historical data\",\n",
    "    \"Grouping customers based on purchasing behavior\",\n",
    "    \"Teaching a robot to play chess\",\n",
    "    \"Identifying spam emails based on labeled examples\",\n",
    "    \"Discovering patterns in genetic data without prior knowledge\",\n",
    "    \"Building a recommendation system for movies\",\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    \"Supervised Learning (Regression)\",\n",
    "    \"Unsupervised Learning (Clustering)\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Supervised Learning (Classification)\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Can be Supervised or Unsupervised, depending on approach\",\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    print(f\"Scenario {i+1}: {scenario}\")\n",
    "    input(\"Your answer (press Enter to reveal): \")\n",
    "    print(f\"Answer: {answers[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've covered:\n",
    "\n",
    "1. **What is Machine Learning**: Teaching machines to learn from data without explicit programming\n",
    "2. **Types of ML**: Supervised, Unsupervised, and Reinforcement Learning\n",
    "3. **Common Algorithms**: Linear Regression, Decision Trees, Neural Networks\n",
    "4. **Key Challenges**: Overfitting and Underfitting\n",
    "\n",
    "Next lesson, we'll dive deeper into Deep Learning and Neural Networks, which power many of today's most advanced AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/) by Andreas Müller & Sarah Guido\n",
    "- [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course) by Google\n",
    "- [Kaggle Courses](https://www.kaggle.com/learn/overview) - Free hands-on ML tutorials\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
