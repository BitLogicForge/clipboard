{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Large Language Models (LLMs) & Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Welcome to the fifth lesson in our AI course! Now that we understand AI basics, Machine Learning, Deep Learning, and NLP, let's explore Large Language Models and how they can be enhanced through Retrieval-Augmented Generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)\n",
    "\n",
    "### What are Large Language Models?\n",
    "\n",
    "**Large Language Models (LLMs)** are advanced AI systems trained on vast amounts of text data that can understand, generate, and manipulate human language in sophisticated ways.\n",
    "\n",
    "### Key characteristics of LLMs:\n",
    "\n",
    "- **Massive scale**: Trained on billions or trillions of parameters\n",
    "- **Broad knowledge**: Learn from diverse sources spanning the internet, books, and articles\n",
    "- **Few-shot learning**: Can perform new tasks with minimal examples\n",
    "- **Versatility**: Can handle a wide range of language tasks\n",
    "\n",
    "### Popular LLMs:\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**: OpenAI's series of models\n",
    "- **Gemini**: Google's multimodal language model\n",
    "- **Llama**: Meta's open-source LLM family\n",
    "- **BERT**: Google's bidirectional encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LLMs Work\n",
    "\n",
    "At their core, LLMs are based on the **Transformer architecture** we explored in the previous lesson. Let's look at the key components of LLM operation:\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "Before processing text, LLMs break it down into tokens (words, parts of words, or characters).\n",
    "\n",
    "```\n",
    "Example: \"I love machine learning!\" â†’ [\"I\", \"love\", \"machine\", \"learning\", \"!\"]\n",
    "```\n",
    "\n",
    "### 2. Context Window\n",
    "\n",
    "The **context window** defines how much text the model can \"see\" and consider at once.\n",
    "\n",
    "### 3. Next-Token Prediction\n",
    "\n",
    "LLMs fundamentally work by predicting the next token in a sequence based on all previous tokens.\n",
    "\n",
    "### 4. Attention Mechanism\n",
    "\n",
    "The **attention mechanism** allows the model to focus on different parts of the input text when generating each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Large language models (LLMs) can understand and generate human language!\n",
      "Tokens: ['Large', 'language', 'models', '(', 'LLMs', ')', 'can', 'understand', 'and', 'generate', 'human', 'language', '!']\n",
      "Number of tokens: 13\n"
     ]
    }
   ],
   "source": [
    "# Simple demonstration of tokenization\n",
    "import re\n",
    "\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"A very simple tokenizer that splits on spaces and punctuation\"\"\"\n",
    "    # Replace punctuation with spaces around them\n",
    "    for punct in \".,;:!?()[]{}\":\n",
    "        text = text.replace(punct, f\" {punct} \")\n",
    "    # Split on whitespace and filter out empty tokens\n",
    "    return [token for token in text.split() if token]\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"Large language models (LLMs) can understand and generate human language!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_tokenizer(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Capabilities and Limitations\n",
    "\n",
    "### Capabilities:\n",
    "\n",
    "- **Text Generation**: Writing essays, stories, code, and creative content\n",
    "- **Conversation**: Powering chatbots and virtual assistants\n",
    "- **Summarization**: Condensing long documents while preserving key information\n",
    "- **Translation**: Converting text between languages\n",
    "- **Code Generation**: Writing and explaining programming code\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Hallucinations**: Generating false information that sounds plausible\n",
    "2. **Knowledge Cutoff**: Only having information available up to their training date\n",
    "3. **Context Window Limits**: Having a finite amount of context they can consider\n",
    "4. **Bias**: Reflecting biases present in training data\n",
    "5. **Computational Costs**: Requiring significant resources to train and run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is an approach that combines large language models with external knowledge retrieval to improve response accuracy and reduce hallucinations.\n",
    "\n",
    "### How RAG Works\n",
    "\n",
    "RAG operates through a multi-step process:\n",
    "\n",
    "1. **Indexing**: External knowledge sources are processed and stored in a searchable format.\n",
    "2. **Retrieval**: When a user query is received, relevant information is retrieved from the knowledge base.\n",
    "3. **Augmentation**: The retrieved information is added to the prompt sent to the LLM.\n",
    "4. **Generation**: The LLM generates a response based on both its parametric knowledge and the retrieved information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG in AI?\n",
      "\n",
      "Retrieved documents:\n",
      "  1. Artificial intelligence (AI) is intelligence demonstrated by machines.\n",
      "  2. Machine learning is a subset of AI that allows systems to learn from data.\n",
      "\n",
      "RAG Response: Based on the retrieved information, Artificial intelligence (AI) is intelligence demonstrated by machines.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Query: What are Large Language Models?\n",
      "\n",
      "Retrieved documents:\n",
      "  1. Large Language Models (LLMs) are neural networks trained on vast amounts of text data.\n",
      "  2. Natural Language Processing (NLP) enables machines to understand human language.\n",
      "\n",
      "RAG Response: Based on the retrieved information, Large Language Models (LLMs) are neural networks trained on vast amounts of text data.\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG simulation\n",
    "import random\n",
    "\n",
    "# Our knowledge base - a collection of documents\n",
    "knowledge_base = [\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines.\",\n",
    "    \"Machine learning is a subset of AI that allows systems to learn from data.\",\n",
    "    \"Deep learning uses neural networks with many layers to process complex patterns.\",\n",
    "    \"Natural Language Processing (NLP) enables machines to understand human language.\",\n",
    "    \"Large Language Models (LLMs) are neural networks trained on vast amounts of text data.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines LLMs with external knowledge sources.\",\n",
    "]\n",
    "\n",
    "\n",
    "def simple_search(query, documents):\n",
    "    \"\"\"Very simple search function that finds documents containing query terms\"\"\"\n",
    "    query_terms = query.lower().split()\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        score = sum(1 for term in query_terms if term in doc.lower())\n",
    "        if score > 0:  # If at least one term matches\n",
    "            results.append((doc, score))\n",
    "\n",
    "    # Sort by relevance score\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in results]\n",
    "\n",
    "\n",
    "def simple_llm(prompt):\n",
    "    \"\"\"Very simple LLM simulator that returns pre-defined responses\"\"\"\n",
    "    if \"what is ai\" in prompt.lower():\n",
    "        return \"AI refers to computer systems that can perform tasks normally requiring human intelligence.\"\n",
    "    elif \"what is rag\" in prompt.lower():\n",
    "        return \"RAG stands for Retrieval-Augmented Generation. It's a technique that combines LLMs with external knowledge.\"\n",
    "    else:\n",
    "        return \"I don't have specific information about that question.\"\n",
    "\n",
    "\n",
    "def rag_system(query):\n",
    "    \"\"\"Simple RAG system simulation\"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = simple_search(query, knowledge_base)\n",
    "    print(\"\\nRetrieved documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs[:2]):  # Show top 2 results\n",
    "        print(f\"  {i+1}. {doc}\")\n",
    "\n",
    "    # Step 2: Create augmented prompt with retrieved context\n",
    "    if retrieved_docs:\n",
    "        context = \"\\n\".join(retrieved_docs[:2])\n",
    "        augmented_prompt = f\"Context information:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    else:\n",
    "        augmented_prompt = f\"Question: {query}\"\n",
    "\n",
    "    # Step 3: Generate a response (using our simple LLM simulator)\n",
    "    if retrieved_docs:\n",
    "        response = f\"Based on the retrieved information, {retrieved_docs[0]}\"\n",
    "    else:\n",
    "        response = simple_llm(query)\n",
    "\n",
    "    print(f\"\\nRAG Response: {response}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "rag_system(\"What is RAG in AI?\")\n",
    "print(\"\\n--------------------------\\n\")\n",
    "rag_system(\"What are Large Language Models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of RAG\n",
    "\n",
    "RAG offers several advantages over using LLMs alone:\n",
    "\n",
    "1. **Reduced hallucinations**: Grounds responses in retrieved information\n",
    "2. **Up-to-date information**: Can access information beyond the model's training cutoff\n",
    "3. **Domain-specific knowledge**: Can incorporate specialized knowledge bases\n",
    "4. **Transparency**: Can cite sources for verification\n",
    "5. **Cost-effectiveness**: More efficient than training larger models or fine-tuning\n",
    "\n",
    "## RAG Applications\n",
    "\n",
    "- **Enterprise search systems** connecting LLMs to company knowledge\n",
    "- **Customer support bots** with access to product documentation\n",
    "- **Research assistants** that can find and summarize relevant papers\n",
    "- **Educational tools** that provide accurate, source-backed information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "| Concept                                  | Description                                                |\n",
    "| ---------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Large Language Models (LLMs)**         | Advanced AI systems trained on vast amounts of text data   |\n",
    "| **Tokenization**                         | Process of breaking text into smaller units for processing |\n",
    "| **Context Window**                       | Amount of text an LLM can consider at once                 |\n",
    "| **Hallucinations**                       | When LLMs generate false information that sounds plausible |\n",
    "| **Retrieval-Augmented Generation (RAG)** | Combining LLMs with external knowledge retrieval           |\n",
    "| **Knowledge Base**                       | External information sources used to augment LLM responses |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've explored:\n",
    "\n",
    "1. **Large Language Models (LLMs)** and their capabilities\n",
    "2. **How LLMs work** - tokenization, attention mechanisms, and next-token prediction\n",
    "3. **LLM limitations** including hallucinations and knowledge cutoff\n",
    "4. **Retrieval-Augmented Generation (RAG)** as a solution to enhance LLMs\n",
    "5. **RAG applications** in various domains\n",
    "\n",
    "Understanding these technologies is crucial as they form the foundation of many cutting-edge AI applications today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - The GPT-3 paper\n",
    "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - The original RAG paper\n",
    "- [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html) by Chip Huyen\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
