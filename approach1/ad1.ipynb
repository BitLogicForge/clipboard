{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Language Model for Next Word Prediction\n",
    "\n",
    "This notebook demonstrates the fundamentals of language modeling using PyTorch, including:\n",
    "\n",
    "- Tokenization\n",
    "- Word embeddings\n",
    "- Building and training a neural language model\n",
    "- Next word prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display, HTMLmateri\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Training Sentences\n",
    "\n",
    "We'll start with some simple sentences to train our language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Pet behaviors\n",
    "    \"cat chases small mouse\",\n",
    "    \"dog runs very fast\",\n",
    "    \"cat likes warm milk\",\n",
    "    \"dog barks at strangers\",\n",
    "    \"cats hunt at night\",\n",
    "    \"kittens play with yarn\",\n",
    "    \"puppies chew on toys\",\n",
    "    # Human-pet interactions\n",
    "    \"people love their pets\",\n",
    "    \"children feed the animals\",\n",
    "    \"family adopts new puppy\",\n",
    "    \"woman trains her dog\",\n",
    "    \"man walks his dog\",\n",
    "    # Animal characteristics\n",
    "    \"cats are very independent\",\n",
    "    \"dogs are loyal companions\",\n",
    "    \"birds sing beautiful songs\",\n",
    "    \"rabbits hop around quickly\",\n",
    "    \"fish swim in tanks\",\n",
    "    # Daily activities\n",
    "    \"students study every day\",\n",
    "    \"friends meet for coffee\",\n",
    "    \"runners train each morning\",\n",
    "    \"artists paint colorful pictures\",\n",
    "    \"writers create new stories\",\n",
    "]\n",
    "\n",
    "\n",
    "# Display sentences in a nicely formatted table\n",
    "\n",
    "\n",
    "pd.DataFrame(sentences, columns=[\"Training Sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Tokenization (Mapping Words to Numbers)\n",
    "\n",
    "We'll build a vocabulary from all our sentences and convert words to numerical tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all sentences\n",
    "words = set()\n",
    "for s in sentences:\n",
    "    words.update(s.split())\n",
    "vocab = {word: idx for idx, word in enumerate(sorted(words))}\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Display vocabulary as a formatted table\n",
    "vocab_df = pd.DataFrame(list(vocab.items()), columns=[\"Word\", \"Token ID\"])\n",
    "display(HTML(\"<h3>Vocabulary:</h3>\"))\n",
    "display(vocab_df)\n",
    "\n",
    "# Tokenize all sentences\n",
    "tokenized_sentences = []\n",
    "for s in sentences:\n",
    "    tokens = [vocab[word] for word in s.split()]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "# Display tokenized sentences\n",
    "tokenization_data = []\n",
    "for s, t in zip(sentences, tokenized_sentences):\n",
    "    tokenization_data.append([s, str(t)])\n",
    "\n",
    "display(HTML(\"<h3>Tokenized Sentences:</h3>\"))\n",
    "pd.DataFrame(tokenization_data, columns=[\"Original Sentence\", \"Tokenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive LLM Learning Examples\n",
    "\n",
    "Based on your example that demonstrates next word prediction with visualizations, here are 6 more interactive notebooks to help someone learn about language models:\n",
    "\n",
    "## 1. Text Sentiment Analysis\n",
    "\n",
    "Create an interactive analyzer that takes user input and displays sentiment scores (positive/negative/neutral) with a confidence gauge. Learners will understand how models classify emotional tone.\n",
    "\n",
    "## 2. Text Generation with Parameters\n",
    "\n",
    "Build a notebook where users can enter a prompt and adjust generation parameters (temperature, max tokens, etc.) to see how they affect text completion. Include visualizations of token probabilities.\n",
    "\n",
    "## 3. Named Entity Recognition\n",
    "\n",
    "Develop an interactive tool that highlights entities (people, places, organizations) in user text with different colors. Show confidence scores for each detected entity.\n",
    "\n",
    "## 4. Question-Answering System\n",
    "\n",
    "Create a two-panel interface where users can upload/paste a document in one panel and ask questions in another. Visualize how the model finds and extracts answers from the source text.\n",
    "\n",
    "## 5. Text Summarization Explorer\n",
    "\n",
    "Build a notebook that lets users paste longer text and generate summaries at different lengths/compression rates. Compare extraction vs. abstraction methods.\n",
    "\n",
    "## 6. Semantic Search Visualization\n",
    "\n",
    "Develop a demo where users can search through a document collection using natural language queries. Visualize document embeddings in 2D/3D space to show how semantic similarity works.\n",
    "\n",
    "Each example should follow your interactive pattern with widgets for input, clear visualization of results, and explanatory components that help learners understand what's happening under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Creating Word Embeddings\n",
    "\n",
    "We'll create simple vector representations for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5  # Increased dimension for better representation\n",
    "np.random.seed(42)  # for reproducibility\n",
    "embedding_matrix = np.random.randn(len(vocab), embedding_dim)\n",
    "word_vectors = {word: embedding_matrix[idx] for word, idx in vocab.items()}\n",
    "\n",
    "# Display word vectors in a formatted table\n",
    "vectors_list = []\n",
    "for word, vector in word_vectors.items():\n",
    "    vectors_list.append([word] + list(vector))\n",
    "\n",
    "columns = [\"Word\"] + [f\"Dim {i+1}\" for i in range(embedding_dim)]\n",
    "vectors_df = pd.DataFrame(vectors_list, columns=columns)\n",
    "display(HTML(\"<h3>Word Embeddings:</h3>\"))\n",
    "vectors_df.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Visualizing Word Embeddings\n",
    "\n",
    "Let's create a 2D visualization of our word embeddings using PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Convert embeddings to array for PCA\n",
    "word_list = list(word_vectors.keys())\n",
    "embedding_array = np.array([word_vectors[word] for word in word_list])\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_array)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=\"blue\", alpha=0.5)\n",
    "for i, word in enumerate(word_list):\n",
    "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title(\"2D PCA projection of Word Embeddings\", fontsize=15)\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert embeddings to array for PCA\n",
    "word_list = list(word_vectors.keys())\n",
    "embedding_array = np.array([word_vectors[word] for word in word_list])\n",
    "\n",
    "# Apply KMeans clustering to group similar words\n",
    "n_clusters = 4  # Adjust based on vocabulary size\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embedding_array)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_array)\n",
    "\n",
    "# Plot the embeddings with colors by cluster\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a colormap\n",
    "colors = [\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\", \"#c2c2f0\"]\n",
    "for i in range(n_clusters):\n",
    "    # Select points in this cluster\n",
    "    cluster_points = embeddings_2d[cluster_labels == i]\n",
    "    cluster_words = [word_list[j] for j in range(len(word_list)) if cluster_labels[j] == i]\n",
    "\n",
    "    # Plot points for this cluster\n",
    "    plt.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        alpha=0.7,\n",
    "        label=f\"Cluster {i+1}\",\n",
    "    )\n",
    "\n",
    "    # Add word labels\n",
    "    for j, word in enumerate(cluster_words):\n",
    "        idx = word_list.index(word)\n",
    "        plt.annotate(word, xy=(embeddings_2d[idx, 0], embeddings_2d[idx, 1]), fontsize=12)\n",
    "\n",
    "plt.title(\"2D PCA projection of Word Embeddings with Clustering\", fontsize=15)\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display words in each cluster\n",
    "print(\"Words grouped by similarity:\")\n",
    "for i in range(n_clusters):\n",
    "    cluster_words = [word_list[j] for j in range(len(word_list)) if cluster_labels[j] == i]\n",
    "    print(f\"Cluster {i+1}: {', '.join(cluster_words)}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Neural Network Model\n",
    "\n",
    "Create a simple language model using LSTM for next word prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, sequence_length, hidden_dim)\n",
    "        # Get the last time step output\n",
    "        output = self.fc(lstm_out[:, -1, :])  # (batch_size, vocab_size)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize network with parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 5\n",
    "hidden_dim = 10\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Display model architecture\n",
    "display(HTML(f\"<h3>Model Architecture:</h3>\"))\n",
    "display(HTML(f\"<pre>{model}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Prepare Training Data\n",
    "\n",
    "Format our tokenized sentences into input-output pairs for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(tokenized_sentences):\n",
    "    X, y = [], []\n",
    "    for tokens in tokenized_sentences:\n",
    "        for i in range(1, len(tokens)):\n",
    "            # Use tokens up to i-1 as input, token i as target\n",
    "            X.append(tokens[:i])\n",
    "            y.append(tokens[i])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = prepare_training_data(tokenized_sentences)\n",
    "\n",
    "# Display training examples in a table\n",
    "examples = []\n",
    "for i in range(min(10, len(X))):\n",
    "    input_words = [reverse_vocab[token] for token in X[i]]\n",
    "    target_word = reverse_vocab[y[i]]\n",
    "    examples.append([\" \".join(input_words), target_word])\n",
    "\n",
    "display(HTML(f\"<h3>Training Examples (showing {len(examples)} of {len(X)}):</h3>\"))\n",
    "pd.DataFrame(examples, columns=[\"Input Sequence\", \"Target Word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Training Loop\n",
    "\n",
    "Train the model on our prepared data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Keep track of losses for plotting\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            # Prepare input sequence\n",
    "            seq = torch.tensor(X[i], dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "            target = torch.tensor([y[i]], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(seq)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        epoch_loss = total_loss / len(X)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Train the model and capture losses\n",
    "losses = train_model(model, X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Visualize Training Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, color=\"blue\", linewidth=2)\n",
    "plt.title(\"Training Loss Over Time\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Next Word Prediction Function\n",
    "\n",
    "Create a function to predict the next word given a partial sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, sentence, vocab, reverse_vocab):\n",
    "    # Tokenize the input\n",
    "    tokens = [vocab.get(word, 0) for word in sentence.split()]\n",
    "    seq = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(seq)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_idx = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    # Get top 3 predictions with probabilities\n",
    "    top_probs, top_indices = torch.topk(probabilities, 3, dim=1)\n",
    "    top_words = [\n",
    "        (reverse_vocab[idx.item()], prob.item()) for idx, prob in zip(top_indices[0], top_probs[0])\n",
    "    ]\n",
    "\n",
    "    return reverse_vocab[predicted_idx], top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣0️⃣ Test the Model with Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = [\"cat chases\", \"dog runs\", \"cat\", \"dog\", \"people love\"]\n",
    "\n",
    "results = []\n",
    "for phrase in test_phrases:\n",
    "    next_word, top_words = predict_next_word(model, phrase, vocab, reverse_vocab)\n",
    "\n",
    "    # Format top predictions with probabilities\n",
    "    top_predictions = \", \".join([f\"{word} ({prob:.2f})\" for word, prob in top_words])\n",
    "    results.append([phrase, next_word, top_predictions])\n",
    "\n",
    "# Display results in a nice table\n",
    "display(HTML(\"<h3>Next Word Predictions:</h3>\"))\n",
    "pd.DataFrame(\n",
    "    results, columns=[\"Input Phrase\", \"Top Prediction\", \"Top 3 Predictions (with probabilities)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣1️⃣ Visualize Prediction Probabilities for a Sample Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample phrase\n",
    "sample_phrase = \"cat chases\"\n",
    "\n",
    "# Get full probability distribution\n",
    "tokens = [vocab.get(word, 0) for word in sample_phrase.split()]\n",
    "seq = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model(seq)\n",
    "    probabilities = torch.softmax(output, dim=1)[0].numpy()\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "prob_data = [(reverse_vocab[i], float(probabilities[i])) for i in range(len(vocab))]\n",
    "prob_df = pd.DataFrame(prob_data, columns=[\"Word\", \"Probability\"])\n",
    "prob_df = prob_df.sort_values(\"Probability\", ascending=False)\n",
    "\n",
    "# Get only top 5 results\n",
    "top5_df = prob_df.head(5)\n",
    "\n",
    "# Plot the top 5 probabilities\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=\"Probability\", y=\"Word\", data=top5_df, palette=\"viridis\")\n",
    "plt.title(f'Top 5 Prediction Probabilities After \"{sample_phrase}\"', fontsize=15)\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.xlim(0, max(top5_df[\"Probability\"]) * 1.1)  # Set x-axis limit with some padding\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top 5 predictions with probabilities\n",
    "print(\"Top 5 predicted next words:\")\n",
    "for i, row in top5_df.iterrows():\n",
    "    print(f\"{row['Word']}: {row['Probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣2️⃣ Interactive Next Word Prediction\n",
    "\n",
    "Try your own phrases to see what the model predicts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "text_input = widgets.Text(\n",
    "    value=\"\", placeholder=\"Type a phrase\", description=\"Input:\", disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Predict Next Word\")\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        phrase = text_input.value\n",
    "        if phrase.strip() == \"\":\n",
    "            print(\"Please enter a phrase\")\n",
    "            return\n",
    "\n",
    "        # Check if words are in vocabulary\n",
    "        unknown_words = [word for word in phrase.split() if word not in vocab]\n",
    "        if unknown_words:\n",
    "            print(f\"Warning: Unknown words in input: {', '.join(unknown_words)}\")\n",
    "            print(\"These will be treated as the first word in vocab for prediction.\")\n",
    "\n",
    "        next_word, top_words = predict_next_word(model, phrase, vocab, reverse_vocab)\n",
    "        print(f\"Input: '{phrase}'\")\n",
    "        print(f\"Predicted next word: '{next_word}'\")\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for word, prob in top_words:\n",
    "            print(f\"  - {word} (probability: {prob:.4f})\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(text_input, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_input = widgets.Text(\n",
    "    value=\"\", placeholder=\"Type a phrase\", description=\"Input:\", disabled=False\n",
    ")\n",
    "clear_button = widgets.Button(description=\"Clear\")\n",
    "output = widgets.Output()\n",
    "status = widgets.HTML(value=\"\")\n",
    "\n",
    "\n",
    "def predict_with_debounce(change):\n",
    "    # Add a short delay to avoid excessive predictions while typing\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    with output:\n",
    "        clear_output()\n",
    "        phrase = change[\"new\"].lower().strip()  # Convert to lowercase\n",
    "\n",
    "        if not phrase:\n",
    "            status.value = \"\"\n",
    "            return\n",
    "\n",
    "        status.value = \"<i>Predicting...</i>\"\n",
    "\n",
    "        # Remove punctuation for better matching\n",
    "        import re\n",
    "\n",
    "        clean_phrase = re.sub(r\"[^\\w\\s]\", \"\", phrase)\n",
    "\n",
    "        # Check vocabulary\n",
    "        unknown_words = [word for word in clean_phrase.split() if word not in vocab]\n",
    "\n",
    "        next_word, top_words = predict_next_word(model, clean_phrase, vocab, reverse_vocab)\n",
    "\n",
    "        # Display results with better formatting\n",
    "        print(f\"<b>Input:</b> '{phrase}'\")\n",
    "        if unknown_words:\n",
    "            print(f\"<b>Warning:</b> Unknown words: {', '.join(unknown_words)}\")\n",
    "\n",
    "        print(f\"<b>Predicted next word:</b> '{next_word}'\")\n",
    "        print(\"<b>Top predictions:</b>\")\n",
    "\n",
    "        # Visualize probabilities\n",
    "        words, probs = zip(*top_words)\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.bar(words, probs)\n",
    "        plt.title(\"Prediction Probabilities\")\n",
    "        plt.show()\n",
    "\n",
    "        status.value = \"\"\n",
    "\n",
    "\n",
    "def on_clear_clicked(b):\n",
    "    text_input.value = \"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "    status.value = \"\"\n",
    "\n",
    "\n",
    "# Use observe with debounce instead of button click\n",
    "text_input.observe(predict_with_debounce, names=\"value\")\n",
    "clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "display(text_input, clear_button, status, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
