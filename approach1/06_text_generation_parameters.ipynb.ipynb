{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Adjustable Parameters\n",
    "\n",
    "This notebook demonstrates how different parameters affect text generation, including temperature, max token length, and sampling methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n",
      "Model loaded! Using cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading GPT-2 model...\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded! Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Generation Parameters\n",
    "\n",
    "Key parameters that affect text generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Controls randomness. Higher values (&gt;1.0) make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Max Length</td>\n",
       "      <td>Maximum number of tokens to generate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top-k</td>\n",
       "      <td>Only sample from the top k most likely next to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top-p</td>\n",
       "      <td>Sample from the smallest set of tokens whose c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Repetition Penalty</td>\n",
       "      <td>Penalizes repeated tokens to reduce repetition.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Parameter                                        Description\n",
       "0         Temperature  Controls randomness. Higher values (>1.0) make...\n",
       "1          Max Length              Maximum number of tokens to generate.\n",
       "2               Top-k  Only sample from the top k most likely next to...\n",
       "3               Top-p  Sample from the smallest set of tokens whose c...\n",
       "4  Repetition Penalty    Penalizes repeated tokens to reduce repetition."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = [\n",
    "    [\n",
    "        \"Temperature\",\n",
    "        \"Controls randomness. Higher values (>1.0) make output more random, lower values (0.2-0.5) make it more focused.\",\n",
    "    ],\n",
    "    [\"Max Length\", \"Maximum number of tokens to generate.\"],\n",
    "    [\"Top-k\", \"Only sample from the top k most likely next tokens.\"],\n",
    "    [\n",
    "        \"Top-p\",\n",
    "        \"Sample from the smallest set of tokens whose cumulative probability exceeds probability p.\",\n",
    "    ],\n",
    "    [\"Repetition Penalty\", \"Penalizes repeated tokens to reduce repetition.\"],\n",
    "]\n",
    "display(pd.DataFrame(parameters, columns=[\"Parameter\", \"Description\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Probability Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probabilities(input_text, top_k=10):\n",
    "    \"\"\"Get probability distribution for the next token after the input text.\"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "\n",
    "    topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "    topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "\n",
    "    return topk_tokens, topk_probs.cpu().numpy()\n",
    "\n",
    "\n",
    "def plot_token_probabilities(tokens, probs, temperature=1.0):\n",
    "    \"\"\"Plot token probabilities with temperature adjustment.\"\"\"\n",
    "    if temperature != 1.0:\n",
    "        # Simulate temperature effect\n",
    "        probs_temp = probs ** (1.0 / temperature)\n",
    "        probs_temp = probs_temp / probs_temp.sum()  # Renormalize\n",
    "    else:\n",
    "        probs_temp = probs\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(len(tokens))\n",
    "    width = 0.35\n",
    "\n",
    "    if temperature != 1.0:\n",
    "        plt.bar(x - width / 2, probs, width, label=\"Original\", color=\"blue\", alpha=0.7)\n",
    "        plt.bar(\n",
    "            x + width / 2, probs_temp, width, label=f\"Temp={temperature}\", color=\"red\", alpha=0.7\n",
    "        )\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.bar(x, probs, width, color=\"blue\", alpha=0.7)\n",
    "\n",
    "    plt.title(\n",
    "        f'Next Token Probabilities{\" (Temperature = \" + str(temperature) + \")\" if temperature != 1.0 else \"\"}',\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.xlabel(\"Tokens\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(x, tokens, rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return probs_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt, max_length=50, temperature=1.0, top_k=0, top_p=0.9, repetition_penalty=1.0\n",
    "):\n",
    "    \"\"\"Generate text with adjustable parameters.\"\"\"\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    # Generate text\n",
    "\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids,\n",
    "\n",
    "        max_length=len(input_ids[0]) + max_length,\n",
    "        temperature=temperature,\n",
    "\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Decode the output\n",
    "\n",
    "\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    # Extract just the generated part (after the prompt)\n",
    "\n",
    "\n",
    "    if generated_text.startswith(prompt):\n",
    "\n",
    "\n",
    "        generated_text = generated_text[len(prompt) :]\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Text Generation Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>ðŸ”® Text Generation with Parameter Tuning</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5144777b2d54349ae07955eee71a75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='The future of AI technology will', description='Prompt:', layout=Layout(height='80px', width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0544a94b5a4bb7b4407a7398c3a44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=0.7, continuous_update=False, description='Temperature:', layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6d29cf691d46068724b68be99f0c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Text', icon='play', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e03b869991495ab18df6dca3cebe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c997a1559af84ef5a13543dfe56c5a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create widgets for parameter controls\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"The future of AI technology will\",\n",
    "    placeholder=\"Enter your prompt here...\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=widgets.Layout(width=\"90%\", height=\"80px\"),\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Temperature:\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "max_length_slider = widgets.IntSlider(\n",
    "    value=30,\n",
    "    min=10,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description=\"Max Tokens:\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "top_k_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=50,\n",
    "    step=5,\n",
    "    description=\"Top-K:\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description=\"Top-P:\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "repetition_slider = widgets.FloatSlider(\n",
    "    value=1.2,\n",
    "    min=1.0,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Rep. Penalty:\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "show_probs_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Show token probabilities\",\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate Text\", button_style=\"success\", icon=\"play\")\n",
    "\n",
    "output = widgets.Output()\n",
    "status = widgets.HTML(value=\"\")\n",
    "\n",
    "\n",
    "def on_generate_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        status.value = \"<i>Generating...</i>\"\n",
    "\n",
    "        prompt = prompt_input.value.strip()\n",
    "        if not prompt:\n",
    "            status.value = \"<span style='color:red'>Please enter a prompt</span>\"\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Generate text\n",
    "            generated_text = generate_text(\n",
    "                prompt,\n",
    "                max_length=max_length_slider.value,\n",
    "                temperature=temperature_slider.value,\n",
    "                top_k=top_k_slider.value,\n",
    "                top_p=top_p_slider.value,\n",
    "                repetition_penalty=repetition_slider.value,\n",
    "            )\n",
    "\n",
    "            # Display results\n",
    "            display(HTML(f\"<h3>Generated Text:</h3>\"))\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"<p><b>Prompt:</b> <span style='background-color:#e6f7ff'>{prompt}</span></p>\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"<p><b>Completion:</b> <span style='background-color:#f6ffe6'>{generated_text}</span></p>\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Parameter summary\n",
    "            params = f\"Temperature={temperature_slider.value}, Top-K={top_k_slider.value if top_k_slider.value > 0 else 'Off'}, Top-P={top_p_slider.value}, Repetition Penalty={repetition_slider.value}\"\n",
    "            display(HTML(f\"<p><b>Parameters:</b> {params}</p>\"))\n",
    "\n",
    "            # Show token probabilities if checked\n",
    "            if show_probs_checkbox.value:\n",
    "                tokens, probs = get_token_probabilities(prompt, top_k=10)\n",
    "                display(HTML(\"<h4>Next Token Probabilities:</h4>\"))\n",
    "                plot_token_probabilities(tokens, probs, temperature=temperature_slider.value)\n",
    "\n",
    "            status.value = \"<span style='color:green'>Generation complete!</span>\"\n",
    "\n",
    "        except Exception as e:\n",
    "            status.value = f\"<span style='color:red'>Error: {str(e)}</span>\"\n",
    "\n",
    "\n",
    "# Connect the button to the function\n",
    "generate_button.on_click(on_generate_button_clicked)\n",
    "\n",
    "# Create layout\n",
    "display(HTML(\"<h2>ðŸ”® Text Generation with Parameter Tuning</h2>\"))\n",
    "display(prompt_input)\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([temperature_slider, max_length_slider]),\n",
    "            widgets.HBox([top_k_slider, top_p_slider]),\n",
    "            widgets.HBox([repetition_slider, show_probs_checkbox]),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "display(generate_button)\n",
    "display(status)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Recommendations\n",
    "\n",
    "Based on experiments, here are recommended settings for different types of text generation tasks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Parameter Recommendations by Task</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task Type</th>\n",
       "      <th>Recommended Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Creative Writing</td>\n",
       "      <td>Higher temperature (0.8-1.2), moderate top-p (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Factual Content</td>\n",
       "      <td>Lower temperature (0.3-0.6), lower top-p (0.8)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Code Generation</td>\n",
       "      <td>Lower temperature (0.2-0.5), use top-k (10-40)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dialogue</td>\n",
       "      <td>Moderate temperature (0.6-0.9), high top-p (0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Task Type                             Recommended Parameters\n",
       "0  Creative Writing  Higher temperature (0.8-1.2), moderate top-p (...\n",
       "1   Factual Content  Lower temperature (0.3-0.6), lower top-p (0.8)...\n",
       "2   Code Generation  Lower temperature (0.2-0.5), use top-k (10-40)...\n",
       "3          Dialogue  Moderate temperature (0.6-0.9), high top-p (0...."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommendations = [\n",
    "    [\n",
    "        \"Creative Writing\",\n",
    "        \"Higher temperature (0.8-1.2), moderate top-p (0.9), low repetition penalty (1.0-1.1)\",\n",
    "    ],\n",
    "    [\n",
    "        \"Factual Content\",\n",
    "        \"Lower temperature (0.3-0.6), lower top-p (0.8), higher repetition penalty (1.2-1.5)\",\n",
    "    ],\n",
    "    [\n",
    "        \"Code Generation\",\n",
    "        \"Lower temperature (0.2-0.5), use top-k (10-40) instead of top-p, high repetition penalty (1.5+)\",\n",
    "    ],\n",
    "    [\n",
    "        \"Dialogue\",\n",
    "        \"Moderate temperature (0.6-0.9), high top-p (0.95), moderate repetition penalty (1.1-1.3)\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "display(HTML(\"<h3>Parameter Recommendations by Task</h3>\"))\n",
    "display(pd.DataFrame(recommendations, columns=[\"Task Type\", \"Recommended Parameters\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
