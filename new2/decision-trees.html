<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Decision Trees and Random Forests: Interactive Guide</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <link rel="stylesheet" href="css/styles.css" />
    <!-- Removed inline <style> block -->
  </head>
  <body>
    <nav class="top-navigation">
      <div class="nav-container">
        <div class="logo">
          <a href="index.html">AI Learning</a>
        </div>
        <div class="nav-links">
          <a href="index.html">Home</a>
          <a href="perceptron.html">Perceptron</a>
          <a href="neural-networks.html">Neural Networks</a>
          <a href="decision-trees.html" class="active">Decision Trees</a>
          <!-- Removed CNN link -->
        </div>
      </div>
    </nav>

    <div class="container">
      <h1>Decision Trees and Random Forests: Interactive Guide</h1>

      <div class="prerequisite">
        <h3>Prerequisites</h3>
        <p>Before starting this lesson, it's helpful to understand:</p>
        <ul>
          <li>Basic probability concepts</li>
          <li>The concept of data classification</li>
          <li>How to interpret simple visualizations</li>
        </ul>
        <p>
          No programming experience or advanced math is required for this
          lesson!
        </p>
      </div>

      <div class="beginner-note">
        <h3>Beginner's Note</h3>
        <p>
          A decision tree is like a flowchart that helps make decisions by
          asking a series of yes/no questions. Imagine playing a game of "20
          Questions" to identify an animal - each question narrows down the
          possibilities until you reach the answer. Decision trees work the same
          way with data!
        </p>
      </div>

      <div class="section">
        <h2>1. Understanding Decision Trees</h2>
        <p>
          Decision trees are versatile machine learning algorithms that can
          perform both classification and regression tasks. They work by
          recursively splitting the data based on feature values, creating a
          tree-like structure of decisions that leads to predictions.
        </p>

        <div class="concept">
          <h3>Key Concept: How Decision Trees Work</h3>
          <p>
            Decision trees make predictions by navigating from the root node to
            a leaf node through a series of decisions:
          </p>
          <ul>
            <li>
              <strong>Root Node:</strong> The starting point, contains the
              entire dataset
            </li>
            <li>
              <strong>Decision Nodes:</strong> Split the data based on feature
              values
            </li>
            <li><strong>Leaf Nodes:</strong> Contain the final predictions</li>
            <li>
              <strong>Splitting Criteria:</strong> Metrics like Gini impurity or
              information gain determine the best splits
            </li>
          </ul>
        </div>

        <div class="model-diagram">
          <div id="tree-diagram-simple" class="diagram-container"></div>
          <p>
            A simple decision tree for predicting whether to play tennis based
            on weather conditions
          </p>
        </div>
      </div>

      <div class="section">
        <h2>2. Interactive Decision Tree Builder</h2>

        <div class="beginner-note">
          <p>
            In this interactive tool, you can build and visualize decision trees
            for different datasets. Try changing the parameters to see how they
            affect the tree's structure and prediction accuracy.
          </p>
        </div>

        <div class="two-columns">
          <div class="column">
            <h3>Dataset Selection</h3>
            <div class="controls">
              <button id="dataset-circles">Circles Dataset</button>
              <button id="dataset-xor">XOR Dataset</button>
              <button id="dataset-spiral">Spiral Dataset</button>
              <button id="dataset-random">Random Dataset</button>
            </div>
            <div id="dataset-canvas-container">
              <canvas id="dataset-canvas" width="300" height="300"></canvas>
            </div>
          </div>

          <div class="column">
            <h3>Tree Parameters</h3>
            <div class="weight-control">
              <label for="max-depth">Max Depth:</label>
              <input
                type="range"
                id="max-depth"
                min="1"
                max="10"
                step="1"
                value="3"
              />
              <span id="max-depth-value">3</span>
              <p class="parameter-explanation">
                Controls how many levels deep the tree can grow. Deeper trees
                can capture more detail but may overfit.
              </p>
            </div>
            <div class="weight-control">
              <label for="min-samples-split">Min Samples to Split:</label>
              <input
                type="range"
                id="min-samples-split"
                min="2"
                max="20"
                step="1"
                value="5"
              />
              <span id="min-samples-split-value">5</span>
              <p class="parameter-explanation">
                The minimum number of samples required at a node before it can
                be split.
              </p>
            </div>
            <div class="weight-control">
              <label for="impurity-measure">Impurity Measure:</label>
              <select id="impurity-measure">
                <option value="gini">Gini Impurity</option>
                <option value="entropy">Entropy</option>
              </select>
              <p class="parameter-explanation">
                The method used to decide the best splits in the tree.
              </p>
            </div>
            <button id="build-tree-btn" class="action-button">
              Build Decision Tree
            </button>
          </div>
        </div>

        <div class="visualization-container">
          <h3>Decision Tree Visualization</h3>
          <div id="decision-boundary-container">
            <canvas
              id="decision-boundary-canvas"
              width="400"
              height="400"
            ></canvas>
          </div>
          <div id="tree-structure-container">
            <div id="tree-structure"></div>
          </div>
        </div>

        <div class="info">
          <h3>Tree Information</h3>
          <div id="tree-info"></div>
          <div id="node-details">Click on a node to see its details</div>
        </div>
      </div>

      <div class="section">
        <h2>3. How Trees Make Decisions</h2>

        <div class="beginner-note">
          <p>
            This section shows how a decision tree follows a path to make
            predictions. Watch how each data point travels down the tree,
            answering questions at each branch until it reaches a final
            decision.
          </p>
        </div>

        <div class="interactive-example">
          <h3>Follow a Sample Through the Tree</h3>
          <div class="two-columns">
            <div class="column">
              <div class="sample-input">
                <h4>Enter Sample Features</h4>
                <div id="feature-inputs"></div>
                <button id="predict-btn">Predict</button>
              </div>
            </div>
            <div class="column">
              <div id="prediction-path-container">
                <h4>Decision Path</h4>
                <div id="prediction-path">
                  Enter feature values and click "Predict" to see the decision
                  path
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="concept">
          <h3>Splitting Criteria Explained</h3>
          <div class="two-columns">
            <div class="column">
              <h4>Gini Impurity</h4>
              <p>
                Measures the probability of incorrect classification if randomly
                picked:
              </p>
              <div class="formula">Gini = 1 - Σ(p<sub>i</sub>)<sup>2</sup></div>
              <p>
                Where p<sub>i</sub> is the proportion of samples that belong to
                class i
              </p>
              <div class="beginner-explanation">
                <p>
                  Think of Gini impurity as measuring how mixed the data is. A
                  value of 0 means all samples are of the same class (pure),
                  while higher values indicate more mixing.
                </p>
              </div>
            </div>
            <div class="column">
              <h4>Entropy</h4>
              <p>Measures the level of disorder or uncertainty:</p>
              <div class="formula">
                Entropy = -Σ p<sub>i</sub> log<sub>2</sub>(p<sub>i</sub>)
              </div>
              <p>
                A value of 0 means perfect purity, higher values mean more mixed
                classes
              </p>
              <div class="beginner-explanation">
                <p>
                  Entropy is like measuring surprise. If all samples are the
                  same, there's no surprise (entropy = 0). If the data is
                  perfectly mixed (50/50), the surprise is at maximum.
                </p>
              </div>
            </div>
          </div>
          <div id="impurity-visualization"></div>
        </div>
      </div>

      <div class="section">
        <h2>4. From Decision Trees to Random Forests</h2>

        <div class="beginner-note">
          <p>
            A random forest is like asking multiple experts the same question
            and taking a vote on the answer. While one decision tree might make
            mistakes, combining many trees often leads to a more accurate result
            - this is the power of random forests!
          </p>
        </div>

        <p>
          While decision trees are intuitive and easy to interpret, they tend to
          overfit the training data. Random Forests overcome this limitation by
          combining multiple decision trees to create a more robust and accurate
          model.
        </p>

        <div class="two-columns">
          <div class="column">
            <h3>Random Forest Parameters</h3>
            <div class="weight-control">
              <label for="n-trees">Number of Trees:</label>
              <input
                type="range"
                id="n-trees"
                min="1"
                max="50"
                step="1"
                value="10"
              />
              <span id="n-trees-value">10</span>
              <p class="parameter-explanation">
                How many different trees to include in the forest. More trees
                generally improves accuracy but takes longer to train.
              </p>
            </div>
            <div class="weight-control">
              <label for="max-features">Max Features per Tree:</label>
              <select id="max-features">
                <option value="sqrt">Square Root</option>
                <option value="log2">Log2</option>
                <option value="all">All Features</option>
              </select>
              <p class="parameter-explanation">
                How many features each tree considers when making splits.
                Limiting features increases tree diversity.
              </p>
            </div>
            <div class="weight-control">
              <label for="bootstrap">Use Bootstrap Sampling:</label>
              <input type="checkbox" id="bootstrap" checked />
              <p class="parameter-explanation">
                Whether to randomly sample data with replacement when building
                each tree.
              </p>
            </div>
            <button id="build-forest-btn">Build Random Forest</button>
          </div>
          <div class="column">
            <h3>Forest Performance</h3>
            <div id="forest-stats"></div>
            <div id="performance-comparison"></div>
          </div>
        </div>

        <div class="visualization-container">
          <h3>Random Forest Visualization</h3>
          <div id="forest-boundary-container">
            <canvas
              id="forest-boundary-canvas"
              width="400"
              height="400"
            ></canvas>
          </div>
          <div id="forest-structure-container">
            <div id="forest-sample-trees"></div>
          </div>
        </div>

        <div class="concept">
          <h3>Key Techniques in Random Forests</h3>
          <ul>
            <li>
              <strong>Bagging (Bootstrap Aggregating):</strong> Each tree is
              trained on a random subset of the data, sampled with replacement
            </li>
            <li>
              <strong>Feature Randomness:</strong> Each tree considers only a
              random subset of features at each split
            </li>
            <li>
              <strong>Voting/Averaging:</strong> Final prediction is determined
              by majority vote (classification) or averaging (regression)
            </li>
          </ul>
          <div id="bagging-visualization"></div>
        </div>
      </div>

      <div class="section">
        <h2>5. Advantages and Applications</h2>

        <div class="two-columns">
          <div class="column">
            <h3>Advantages</h3>
            <ul>
              <li>
                <strong>Interpretability:</strong> Decision trees provide clear
                decision paths
              </li>
              <li>
                <strong>Handles Mixed Data:</strong> Works with numerical and
                categorical features
              </li>
              <li>
                <strong>Feature Importance:</strong> Naturally ranks feature
                importance
              </li>
              <li>
                <strong>Non-parametric:</strong> No assumptions about data
                distribution
              </li>
              <li>
                <strong>Random Forests:</strong> Reduce overfitting and improve
                accuracy
              </li>
            </ul>
          </div>
          <div class="column">
            <h3>Real-world Applications</h3>
            <ul>
              <li>
                <strong>Banking:</strong> Credit risk assessment and fraud
                detection
              </li>
              <li>
                <strong>Healthcare:</strong> Disease diagnosis and patient
                triage
              </li>
              <li>
                <strong>Marketing:</strong> Customer segmentation and churn
                prediction
              </li>
              <li>
                <strong>Ecology:</strong> Species identification and habitat
                analysis
              </li>
              <li>
                <strong>Finance:</strong> Stock market prediction and portfolio
                management
              </li>
            </ul>
          </div>
        </div>

        <div class="feature-importance-container">
          <h3>Feature Importance Analysis</h3>
          <div id="feature-importance-chart"></div>
        </div>
      </div>

      <div class="section">
        <h2>6. Quiz: Test Your Understanding</h2>
        <div id="decision-tree-quiz">
          <div class="quiz-question">
            <p>
              <strong
                >1. What is the main difference between a decision tree and a
                random forest?</strong
              >
            </p>
            <div class="quiz-options">
              <label
                ><input type="radio" name="q1" value="a" /> Decision trees are
                faster but random forests are more accurate</label
              ><br />
              <label
                ><input type="radio" name="q1" value="b" /> Random forests
                combine multiple decision trees to make predictions</label
              ><br />
              <label
                ><input type="radio" name="q1" value="c" /> Random forests can
                only handle numerical data</label
              ><br />
              <label
                ><input type="radio" name="q1" value="d" /> Decision trees can
                only make binary decisions</label
              >
            </div>
            <div
              class="quiz-feedback"
              id="feedback-q1"
              style="display: none"
            ></div>
          </div>

          <div class="quiz-question">
            <p>
              <strong
                >2. What does "Gini impurity" measure in a decision
                tree?</strong
              >
            </p>
            <div class="quiz-options">
              <label
                ><input type="radio" name="q2" value="a" /> The total number of
                features used in the tree</label
              ><br />
              <label
                ><input type="radio" name="q2" value="b" /> How deep the tree
                has grown</label
              ><br />
              <label
                ><input type="radio" name="q2" value="c" /> The probability of
                incorrect classification</label
              ><br />
              <label
                ><input type="radio" name="q2" value="d" /> The training time
                required for the model</label
              >
            </div>
            <div
              class="quiz-feedback"
              id="feedback-q2"
              style="display: none"
            ></div>
          </div>

          <div class="quiz-question">
            <p>
              <strong
                >3. Why is bootstrapping (random sampling with replacement) used
                in random forests?</strong
              >
            </p>
            <div class="quiz-options">
              <label
                ><input type="radio" name="q3" value="a" /> To speed up the
                training process</label
              ><br />
              <label
                ><input type="radio" name="q3" value="b" /> To create diversity
                among the trees in the forest</label
              ><br />
              <label
                ><input type="radio" name="q3" value="c" /> To reduce the memory
                requirements</label
              ><br />
              <label
                ><input type="radio" name="q3" value="d" /> To allow processing
                of missing values in the data</label
              >
            </div>
            <div
              class="quiz-feedback"
              id="feedback-q3"
              style="display: none"
            ></div>
          </div>

          <button id="submit-quiz">Check Answers</button>
          <div id="quiz-result" style="margin-top: 20px"></div>
        </div>
      </div>

      <div class="section">
        <h2>Key Takeaways</h2>
        <ul>
          <li>
            Decision trees make predictions by asking a series of questions
            about features, creating a flowchart-like structure.
          </li>
          <li>
            They split data based on criteria like Gini impurity or entropy to
            create the purest possible leaf nodes.
          </li>
          <li>
            Decision trees are easy to interpret but can easily overfit the
            training data.
          </li>
          <li>
            Random forests combine multiple decision trees (an ensemble) to
            improve accuracy and reduce overfitting.
          </li>
          <li>
            Random forests use techniques like bagging (bootstrap aggregating)
            and feature randomness to create diverse trees.
          </li>
          <li>
            Both methods can handle numerical and categorical data and provide
            feature importance rankings.
          </li>
        </ul>
      </div>

      <div class="lesson-navigation">
        <a href="neural-networks.html" class="previous-lesson"
          >← Previous: Neural Networks</a
        >
        <!-- Updated next lesson link -->
        <a href="index.html" class="next-lesson">Back to Home Page →</a>
      </div>
    </div>

    <!-- Load JavaScript modules -->
    <!-- Corrected JS paths -->
    <script src="js/decision-tree-data.js"></script>
    <script src="js/decision-tree-visualization.js"></script>
    <script src="js/decision-tree-model.js"></script>
    <script src="js/model.js"></script>
    <!-- Assuming shared model logic -->
    <script>
      // Quiz functionality
      document.addEventListener("DOMContentLoaded", function () {
        const submitQuiz = document.getElementById("submit-quiz");
        if (submitQuiz) {
          submitQuiz.addEventListener("click", function () {
            const correctAnswers = {
              q1: "b",
              q2: "c",
              q3: "b",
            };

            let score = 0;

            // Check each question
            for (const question in correctAnswers) {
              const selected = document.querySelector(
                `input[name="${question}"]:checked`
              );
              const feedbackDiv = document.getElementById(
                `feedback-${question}`
              );

              if (selected) {
                if (selected.value === correctAnswers[question]) {
                  feedbackDiv.innerHTML = "Correct! ✓";
                  feedbackDiv.style.color = "green";
                  score++;
                } else {
                  feedbackDiv.innerHTML = "Incorrect ✗";
                  feedbackDiv.style.color = "red";
                }
              } else {
                feedbackDiv.innerHTML = "Please select an answer";
                feedbackDiv.style.color = "orange";
              }

              feedbackDiv.style.display = "block";
            }

            // Display total score
            const resultDiv = document.getElementById("quiz-result");
            resultDiv.innerHTML = `Your score: ${score} out of ${
              Object.keys(correctAnswers).length
            }`;
          });
        }
      });
    </script>
  </body>
</html>
